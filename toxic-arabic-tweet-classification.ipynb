{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import the models needed \n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) \n\n\nimport tensorflow as tf\n\n#get the version of tensorflow\nprint(\"Version: \", tf.__version__)\n#eager mode\n\nprint(\"Eager mode: \", tf.executing_eagerly())\n\nprint(\"GPU is\", \"available\" if tf.config.experimental.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file = \"/kaggle/input/toxic-arabic-tweets-classification/toxic arabic tweets classification.txt\"\ntweets = pd.read_csv(file,sep=\"\\t\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EDA"},{"metadata":{},"cell_type":"markdown","source":" ## show the head of data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" ## show the nullable data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## describe the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the distinct values of classes of tweets"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets['Class'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Getting the number of values"},{"metadata":{"trusted":true},"cell_type":"code","source":"tweets.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualise each class"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nclasses= ['Abusive tweets','Normal tweets','Hate tweets']\nvalues =[len ( tweets[tweets['Class']=='abusive'].index ),len ( tweets[tweets['Class']=='normal'].index ),len ( tweets[tweets['Class']=='normal'].index )]\nplt.title('Occurrences of type of tweets')\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.bar(classes,values)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data preparation"},{"metadata":{},"cell_type":"markdown","source":"We will internsetted to prepare data by fellowing the steps:\n* Decode the value of the class column\n* clean the data of tweets column\n* Decode the tweets column"},{"metadata":{},"cell_type":"markdown","source":"Decode the value of the class column to make it traitable with DL algorithmes"},{"metadata":{"trusted":true},"cell_type":"code","source":"def decodeValues(value):\n    #'abusive', 'normal', 'hate'\n    if value == 'abusive':\n        return 1\n    elif value == 'normal':\n        return 2\n    elif value == 'hate':\n        return 3\n\n#Map each class into a numerical value\ntweets['Class'] = tweets['Class'].apply(decodeValues)\n\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## clean the data of tweets column\nWe will intressted in this section in data cleaning:\nfirst we need to delete all the non alphabetics values: \n*  deduplicate \n*  Removing puctuations\n*  Removing URL data\n*  removing emojies\n"},{"metadata":{},"cell_type":"markdown","source":"## removing emojies"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\" # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## split text by space"},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_white_space(text):\n    text = text.lower().split()\n    return text\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## remove pnctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import string\n\ndef remove_punctuation(text):\n    result = string.punctuation\n    listText=[]\n    for words in text:\n        String =\"\"\n        for word in words:\n            if word not in result:\n                String+=word\n            else:\n                break\n        if (String!=\"\") :\n            listText.append(String)    \n    return listText","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_data(text):\n    text = remove_emoji(text)\n    #text = split_white_space(text)\n    #text = remove_punctuation(text)\n    return text\n\ntweets['Tweet'] = tweets['Tweet'].apply(clean_data)\n\ntweets.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Vectorizing the words using hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split( tweets['Tweet'], tweets['Class'], test_size=0.2)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\nprint('Number of Unique Tokens',len(tokenizer.word_index))\n\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 200\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\nprint( X_train.shape )\nprint(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply LSTM architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\nvocab_size = 10000\nembedding_dim = 1000\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n     model = tf.keras.Sequential([\n        #Word embdading layer (Input layer)\n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.LSTM(50),\n        tf.keras.layers.Dense(embedding_dim, activation='relu'),\n        tf.keras.layers.Dense(140, activation='relu'),\n        tf.keras.layers.Dense(150, activation='relu'),\n        #Output layer(We use softmax activation function in multiple classification)\n        tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train, epochs=50, steps_per_epoch=50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Calcule the accuracy"},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_lost , test_acc = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the model is:\",(test_acc*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(r'./LSTM.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply CNN architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Conv1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\nvocab_size = 1000\nembedding_dim = 100\n\n\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n     model = tf.keras.Sequential([\n        #Word embdading layer (Input layer)\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,input_length=200),\n        Conv1D(filters=128, kernel_size=3, activation='relu',padding=\"valid\"),\n        MaxPooling1D(),\n         Flatten(),\n        #Output layer\n        tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train, epochs=60, steps_per_epoch=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ntest_lost , test_acc_cnn = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the CNN model is:\",(test_acc_cnn*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(r'./CNN.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Apply GRU architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\nvocab_size = 10000\nembedding_dim = 1000\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n     model = tf.keras.Sequential([\n        #Embedding layer(input)\n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.GRU(50),\n        tf.keras.layers.Dense(embedding_dim, activation='relu'),\n        tf.keras.layers.Dense(145, activation='relu'),\n         #output layer\n        tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train, epochs=50, steps_per_epoch=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_lost , test_acc_gru = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the RNN-GRU model is:\",(test_acc_gru*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(r'./GRU.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{},"cell_type":"markdown","source":" # Apply RNN architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate\nvocab_size = 10000\nembedding_dim = 1000\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n     model = tf.keras.Sequential([\n        #embedding layer(input)\n        tf.keras.layers.Embedding(vocab_size, embedding_dim),\n        tf.keras.layers.SimpleRNN(50),\n        tf.keras.layers.Dense(embedding_dim, activation='relu'),\n        tf.keras.layers.Dense(145, activation='relu'),\n         #output layer\n        tf.keras.layers.Dense(4, activation=\"softmax\")\n    ])\n\nmodel.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\" , metrics=[\"accuracy\"])\n# train model normally\nmodel.fit(X_train, y_train, epochs=50, steps_per_epoch=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_lost , test_acc_rnn = model.evaluate(X_test, y_test)\n\n\nprint(\"The accuracy of the RNN model is:\",(test_acc_rnn*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(r'./RNN.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Compare each architecture used"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\narche= ['RNN','CNN','LSTM','GRU']\naccuracy =[(test_acc_rnn*100),(test_acc_cnn*100),(test_acc*100),(test_acc_gru*100)]\nplt.title('Accuracy of architectures')\nplt.ylabel('architectures', fontsize=12)\nplt.bar(arche,accuracy)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}